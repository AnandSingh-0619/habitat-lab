{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20fd9459-8522-471c-8306-e9173a53c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import git\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import habitat\n",
    "from habitat.core.logging import logger\n",
    "from habitat.core.registry import registry\n",
    "from habitat.sims.habitat_simulator.actions import HabitatSimActions\n",
    "from habitat.tasks.nav.nav import NavigationTask\n",
    "from habitat_baselines.common.baseline_registry import baseline_registry\n",
    "from habitat_baselines.config.default import get_config as get_baselines_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 16:35:14,150 Initializing dataset RearrangeDataset-v0\n",
      "Exception ignored in: <function Simulator.__del__ at 0x7f4a5a225430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anand/miniconda3/envs/habitat/lib/python3.9/site-packages/habitat_sim-0.2.5-py3.9-linux-x86_64.egg/habitat_sim/simulator.py\", line 528, in __del__\n",
      "    self.close(destroy=True)\n",
      "  File \"/home/anand/miniconda3/envs/habitat/lib/python3.9/site-packages/habitat_sim-0.2.5-py3.9-linux-x86_64.egg/habitat_sim/simulator.py\", line 131, in close\n",
      "    if not self._initialized:\n",
      "AttributeError: 'RearrangeSim' object has no attribute '_initialized'\n",
      "2024-01-17 16:35:44,603 initializing sim RearrangeSim-v0\n",
      "[16:35:44:730861]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(206)::setValsFromJSONDoc : (Articulated Object) : No Glob path result for `2024-01-17 16:35:48,927 Initializing task RearrangePickTask-v0\n",
      "data/replica_cad/../hab_fetch_1.0/robots/fetch_no_base.urdf`\n",
      "[16:35:44:813690]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_00.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813728]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_01.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813747]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_02.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813766]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_03.navmesh` not found on disk as absolute path or relative to `data/replica_cad`"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renderer: NVIDIA GeForce GTX 1650/PCIe/SSE2 by NVIDIA Corporation\n",
      "OpenGL version: 4.6.0 NVIDIA 545.23.08\n",
      "Using optional features:\n",
      "    GL_ARB_vertex_array_object\n",
      "    GL_ARB_separate_shader_objects\n",
      "    GL_ARB_robustness\n",
      "    GL_ARB_texture_storage\n",
      "    GL_ARB_texture_view\n",
      "    GL_ARB_framebuffer_no_attachments\n",
      "    GL_ARB_invalidate_subdata\n",
      "    GL_ARB_texture_storage_multisample\n",
      "    GL_ARB_multi_bind\n",
      "    GL_ARB_direct_state_access\n",
      "    GL_ARB_get_texture_sub_image\n",
      "    GL_ARB_texture_filter_anisotropic\n",
      "    GL_KHR_debug\n",
      "    GL_KHR_parallel_shader_compile\n",
      "    GL_NV_depth_buffer_float\n",
      "Using driver workarounds:\n",
      "    no-forward-compatible-core-context\n",
      "    nv-egl-incorrect-gl11-function-pointers\n",
      "    no-layout-qualifiers-on-old-glsl\n",
      "    nv-zero-context-profile-mask\n",
      "    nv-implementation-color-read-format-dsa-broken\n",
      "    nv-cubemap-inconsistent-compressed-image-size\n",
      "    nv-cubemap-broken-full-compressed-image-query\n",
      "    nv-compressed-block-size-in-bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[16:35:44:813784]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_04.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813803]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_05.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813821]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_06.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813840]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_07.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813858]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_08.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813877]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_09.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813895]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_10.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813913]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_11.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813932]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_12.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813950]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_13.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813968]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_14.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:813987]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_15.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:814005]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_16.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:814023]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_17.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:814043]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_18.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:814062]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_19.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "[16:35:44:814080]:[Warning]:[Metadata] SceneDatasetAttributesManager.cpp(267)::loadAndValidateMap : `navmesh_instances` Value : `navmeshes/v3_sc4_staging_20.navmesh` not found on disk as absolute path or relative to `data/replica_cad`\n",
      "MeshTools::compile(): ignoring Trade::MeshAttribute::TextureCoordinates 1 as its biding slot is already occupied by Trade::MeshAttribute::TextureCoordinates 0\n",
      "MeshTools::compile(): ignoring Trade::MeshAttribute::TextureCoordinates 1 as its biding slot is already occupied by Trade::MeshAttribute::TextureCoordinates 0\n",
      "MeshTools::compile(): ignoring Trade::MeshAttribute::TextureCoordinates 1 as its biding slot is already occupied by Trade::MeshAttribute::TextureCoordinates 0\n",
      "MeshTools::compile(): ignoring Trade::MeshAttribute::TextureCoordinates 1 as its biding slot is already occupied by Trade::MeshAttribute::TextureCoordinates 0\n",
      "[16:35:45:291148]:[Warning]:[Sim] Simulator.cpp(508)::instanceStageForSceneAttributes : The active scene does not contain semantic annotations : activeSemanticSceneID_ = 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = habitat.get_config(\n",
    "        config_path=\"benchmark/rearrange/pick.yaml\",\n",
    "        overrides=[\n",
    "            \"habitat.environment.max_episode_steps=20\",\n",
    "            \"habitat.environment.iterator_options.shuffle=False\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        env.close()  # type: ignore[has-type]\n",
    "    except NameError:\n",
    "        pass\n",
    "    env = habitat.Env(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.number_of_episodes=[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent acting inside environment.\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 9.787683375179768}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5758212, 'ee_to_object_distance': {'0': 0.75780064}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.00800000037997961, 'did_violate_hold_constraint': False, 'num_steps': 1}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 10.699151083827019}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5705843, 'ee_to_object_distance': {'0': 0.7553139}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': 0.004000000189989805, 'did_violate_hold_constraint': False, 'num_steps': 2}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 12.583854608237743}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5723508, 'ee_to_object_distance': {'0': 0.75715756}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.004000000189989805, 'did_violate_hold_constraint': False, 'num_steps': 3}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 11.32093369960785}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.57997644, 'ee_to_object_distance': {'0': 0.76291394}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.012000000104308128, 'did_violate_hold_constraint': False, 'num_steps': 4}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 10.83111297339201}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58058554, 'ee_to_object_distance': {'0': 0.7756174}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.026000000536441803, 'did_violate_hold_constraint': False, 'num_steps': 5}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 9.797287173569202}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58036566, 'ee_to_object_distance': {'0': 0.78826976}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.026000000536441803, 'did_violate_hold_constraint': False, 'num_steps': 6}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 9.8084956407547}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58034474, 'ee_to_object_distance': {'0': 0.7842128}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': 0.00800000037997961, 'did_violate_hold_constraint': False, 'num_steps': 7}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 10.017964988946915}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58098, 'ee_to_object_distance': {'0': 0.78568405}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.0020000000949949026, 'did_violate_hold_constraint': False, 'num_steps': 8}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 10.334859602153301}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58143896, 'ee_to_object_distance': {'0': 0.7991789}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.026000000536441803, 'did_violate_hold_constraint': False, 'num_steps': 9}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 11.054838076233864}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5813386, 'ee_to_object_distance': {'0': 0.8028344}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.00800000037997961, 'did_violate_hold_constraint': False, 'num_steps': 10}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 10.38809772580862}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58124715, 'ee_to_object_distance': {'0': 0.78943187}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': 0.026000000536441803, 'did_violate_hold_constraint': False, 'num_steps': 11}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 15.204387716948986}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5850011, 'ee_to_object_distance': {'0': 0.7928527}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.006000000052154064, 'did_violate_hold_constraint': False, 'num_steps': 12}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 15.071752481162548}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58047515, 'ee_to_object_distance': {'0': 0.7873083}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': 0.012000000104308128, 'did_violate_hold_constraint': False, 'num_steps': 13}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 15.641821548342705}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58098876, 'ee_to_object_distance': {'0': 0.80078715}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.026000000536441803, 'did_violate_hold_constraint': False, 'num_steps': 14}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 14.404447749257088}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5745539, 'ee_to_object_distance': {'0': 0.79705137}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': 0.00800000037997961, 'did_violate_hold_constraint': False, 'num_steps': 15}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 12.773081660270691}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5837605, 'ee_to_object_distance': {'0': 0.80450565}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.014000000432133675, 'did_violate_hold_constraint': False, 'num_steps': 16}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 11.443725787103176}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5844831, 'ee_to_object_distance': {'0': 0.8185639}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.02800000086426735, 'did_violate_hold_constraint': False, 'num_steps': 17}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 9.914149530231953}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5773768, 'ee_to_object_distance': {'0': 0.8115838}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': 0.014000000432133675, 'did_violate_hold_constraint': False, 'num_steps': 18}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 11.126645840704441}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.5851449, 'ee_to_object_distance': {'0': 0.8185897}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': -0.014000000432133675, 'did_violate_hold_constraint': False, 'num_steps': 19}\n",
      "{'articulated_agent_force': {'accum': 0.0, 'instant': 9.828837588429451}, 'force_terminate': False, 'robot_collisions': {'total_collisions': 0, 'robot_obj_colls': 0, 'robot_scene_colls': 0, 'obj_scene_colls': 0}, 'collision_terminate': False, 'ee_to_rest_distance': 0.58586305, 'ee_to_object_distance': {'0': 0.80517435}, 'did_pick_object': 0, 'pick_success': False, 'pick_reward': 0.026000000536441803, 'did_violate_hold_constraint': False, 'num_steps': 20}\n",
      "Episode finished after 20 steps.\n"
     ]
    }
   ],
   "source": [
    "from habitat_sim.utils import viz_utils as vut\n",
    "from habitat.utils.visualizations.utils import (\n",
    "    observations_to_image,\n",
    "    overlay_frame,\n",
    ")\n",
    "from habitat.tasks.rearrange.utils import (\n",
    "    CollisionDetails,\n",
    "    UsesArticulatedAgentInterface,\n",
    "    batch_transform_point,\n",
    "    get_angle_to_pos,\n",
    "    rearrange_logger,\n",
    ")\n",
    "from habitat.tasks.rearrange.rearrange_sim import RearrangeSim\n",
    "observations = env.reset()  # noqa: F841\n",
    "\n",
    "print(\"Agent acting inside environment.\")\n",
    "count_steps = 0\n",
    "# To save the video\n",
    "video_file_path = \"data/example_interact.mp4\"\n",
    "video_writer = vut.get_fast_video_writer(video_file_path, fps=30)\n",
    "\n",
    "while not env.episode_over:\n",
    "    observations = env.step(env.action_space.sample())  # noqa: F841\n",
    "    info = env.get_metrics()\n",
    "    print(info)\n",
    "    # print(observations)\n",
    "    render_obs = observations_to_image(observations, info)\n",
    "    render_obs = overlay_frame(render_obs, info)\n",
    "\n",
    "    video_writer.append_data(render_obs)\n",
    "\n",
    "    count_steps += 1\n",
    "print(\"Episode finished after {} steps.\".format(count_steps))\n",
    "\n",
    "video_writer.close()\n",
    "if vut.is_notebook():\n",
    "    vut.display_video(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from habitat.tasks.nav.nav import PointGoalSensor\n",
    "class MultiObjSensor(PointGoalSensor):\n",
    "    \"\"\"\n",
    "    Abstract parent class for a sensor that specifies the locations of all targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, task, **kwargs):\n",
    "        self._task = task\n",
    "        self._sim: RearrangeSim\n",
    "        super().__init__(*args, task=task, **kwargs)\n",
    "\n",
    "    def _get_observation_space(self, *args, **kwargs):\n",
    "        n_targets = self._task.get_n_targets()\n",
    "        return spaces.Box(\n",
    "            shape=(n_targets * 3,),\n",
    "            low=np.finfo(np.float32).min,\n",
    "            high=np.finfo(np.float32).max,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "class TargetStartSensor(UsesArticulatedAgentInterface, MultiObjSensor):\n",
    "    \"\"\"\n",
    "    Relative position from end effector to target object\n",
    "    \"\"\"\n",
    "\n",
    "    cls_uuid: str = \"obj_start_sensor\"\n",
    "\n",
    "    def get_observation(self, *args, observations, episode, **kwargs):\n",
    "        self._sim: RearrangeSim\n",
    "        global_T = self._sim.get_agent_data(\n",
    "            self.agent_id\n",
    "        ).articulated_agent.ee_transform()\n",
    "        T_inv = global_T.inverted()\n",
    "        pos = self._sim.get_target_objs_start()\n",
    "        return batch_transform_point(pos, T_inv, np.float32).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['head_depth', 'head_rgb', 'obj_start_sensor', 'joint', 'is_holding', 'ee_pos', 'relative_resting_position'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.keys()\n",
    "# print(observations[\"ee_pos\"])\n",
    "# print(observations[\"relative_resting_position\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(\n",
    "    rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([])):  # noqa: B006\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGB\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\n",
    "            \"P\", (semantic_obs.shape[1], semantic_obs.shape[0])\n",
    "        )\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray(\n",
    "            (depth_obs / 10 * 255).astype(np.uint8), mode=\"L\"\n",
    "        )\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MyPolicy:\n\tMissing key(s) in state_dict: \"conv.weight\", \"conv.bias\", \"resnet.conv1.weight\", \"resnet.bn1.weight\", \"resnet.bn1.bias\", \"resnet.bn1.running_mean\", \"resnet.bn1.running_var\", \"resnet.layer1.0.conv1.weight\", \"resnet.layer1.0.bn1.weight\", \"resnet.layer1.0.bn1.bias\", \"resnet.layer1.0.bn1.running_mean\", \"resnet.layer1.0.bn1.running_var\", \"resnet.layer1.0.conv2.weight\", \"resnet.layer1.0.bn2.weight\", \"resnet.layer1.0.bn2.bias\", \"resnet.layer1.0.bn2.running_mean\", \"resnet.layer1.0.bn2.running_var\", \"resnet.layer1.1.conv1.weight\", \"resnet.layer1.1.bn1.weight\", \"resnet.layer1.1.bn1.bias\", \"resnet.layer1.1.bn1.running_mean\", \"resnet.layer1.1.bn1.running_var\", \"resnet.layer1.1.conv2.weight\", \"resnet.layer1.1.bn2.weight\", \"resnet.layer1.1.bn2.bias\", \"resnet.layer1.1.bn2.running_mean\", \"resnet.layer1.1.bn2.running_var\", \"resnet.layer2.0.conv1.weight\", \"resnet.layer2.0.bn1.weight\", \"resnet.layer2.0.bn1.bias\", \"resnet.layer2.0.bn1.running_mean\", \"resnet.layer2.0.bn1.running_var\", \"resnet.layer2.0.conv2.weight\", \"resnet.layer2.0.bn2.weight\", \"resnet.layer2.0.bn2.bias\", \"resnet.layer2.0.bn2.running_mean\", \"resnet.layer2.0.bn2.running_var\", \"resnet.layer2.0.downsample.0.weight\", \"resnet.layer2.0.downsample.1.weight\", \"resnet.layer2.0.downsample.1.bias\", \"resnet.layer2.0.downsample.1.running_mean\", \"resnet.layer2.0.downsample.1.running_var\", \"resnet.layer2.1.conv1.weight\", \"resnet.layer2.1.bn1.weight\", \"resnet.layer2.1.bn1.bias\", \"resnet.layer2.1.bn1.running_mean\", \"resnet.layer2.1.bn1.running_var\", \"resnet.layer2.1.conv2.weight\", \"resnet.layer2.1.bn2.weight\", \"resnet.layer2.1.bn2.bias\", \"resnet.layer2.1.bn2.running_mean\", \"resnet.layer2.1.bn2.running_var\", \"resnet.layer3.0.conv1.weight\", \"resnet.layer3.0.bn1.weight\", \"resnet.layer3.0.bn1.bias\", \"resnet.layer3.0.bn1.running_mean\", \"resnet.layer3.0.bn1.running_var\", \"resnet.layer3.0.conv2.weight\", \"resnet.layer3.0.bn2.weight\", \"resnet.layer3.0.bn2.bias\", \"resnet.layer3.0.bn2.running_mean\", \"resnet.layer3.0.bn2.running_var\", \"resnet.layer3.0.downsample.0.weight\", \"resnet.layer3.0.downsample.1.weight\", \"resnet.layer3.0.downsample.1.bias\", \"resnet.layer3.0.downsample.1.running_mean\", \"resnet.layer3.0.downsample.1.running_var\", \"resnet.layer3.1.conv1.weight\", \"resnet.layer3.1.bn1.weight\", \"resnet.layer3.1.bn1.bias\", \"resnet.layer3.1.bn1.running_mean\", \"resnet.layer3.1.bn1.running_var\", \"resnet.layer3.1.conv2.weight\", \"resnet.layer3.1.bn2.weight\", \"resnet.layer3.1.bn2.bias\", \"resnet.layer3.1.bn2.running_mean\", \"resnet.layer3.1.bn2.running_var\", \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\", \"resnet.fc.weight\", \"resnet.fc.bias\", \"fc1.weight\", \"fc1.bias\", \"rnn.weight_ih_l0\", \"rnn.weight_hh_l0\", \"rnn.bias_ih_l0\", \"rnn.bias_hh_l0\", \"rnn.weight_ih_l1\", \"rnn.weight_hh_l1\", \"rnn.bias_ih_l1\", \"rnn.bias_hh_l1\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"net.prev_action_embedding.weight\", \"net.tgt_embeding.weight\", \"net.tgt_embeding.bias\", \"net.visual_encoder.backbone.conv1.0.weight\", \"net.visual_encoder.backbone.conv1.1.weight\", \"net.visual_encoder.backbone.conv1.1.bias\", \"net.visual_encoder.backbone.layer1.0.convs.0.weight\", \"net.visual_encoder.backbone.layer1.0.convs.1.weight\", \"net.visual_encoder.backbone.layer1.0.convs.1.bias\", \"net.visual_encoder.backbone.layer1.0.convs.3.weight\", \"net.visual_encoder.backbone.layer1.0.convs.4.weight\", \"net.visual_encoder.backbone.layer1.0.convs.4.bias\", \"net.visual_encoder.backbone.layer1.1.convs.0.weight\", \"net.visual_encoder.backbone.layer1.1.convs.1.weight\", \"net.visual_encoder.backbone.layer1.1.convs.1.bias\", \"net.visual_encoder.backbone.layer1.1.convs.3.weight\", \"net.visual_encoder.backbone.layer1.1.convs.4.weight\", \"net.visual_encoder.backbone.layer1.1.convs.4.bias\", \"net.visual_encoder.backbone.layer2.0.convs.0.weight\", \"net.visual_encoder.backbone.layer2.0.convs.1.weight\", \"net.visual_encoder.backbone.layer2.0.convs.1.bias\", \"net.visual_encoder.backbone.layer2.0.convs.3.weight\", \"net.visual_encoder.backbone.layer2.0.convs.4.weight\", \"net.visual_encoder.backbone.layer2.0.convs.4.bias\", \"net.visual_encoder.backbone.layer2.0.downsample.0.weight\", \"net.visual_encoder.backbone.layer2.0.downsample.1.weight\", \"net.visual_encoder.backbone.layer2.0.downsample.1.bias\", \"net.visual_encoder.backbone.layer2.1.convs.0.weight\", \"net.visual_encoder.backbone.layer2.1.convs.1.weight\", \"net.visual_encoder.backbone.layer2.1.convs.1.bias\", \"net.visual_encoder.backbone.layer2.1.convs.3.weight\", \"net.visual_encoder.backbone.layer2.1.convs.4.weight\", \"net.visual_encoder.backbone.layer2.1.convs.4.bias\", \"net.visual_encoder.backbone.layer3.0.convs.0.weight\", \"net.visual_encoder.backbone.layer3.0.convs.1.weight\", \"net.visual_encoder.backbone.layer3.0.convs.1.bias\", \"net.visual_encoder.backbone.layer3.0.convs.3.weight\", \"net.visual_encoder.backbone.layer3.0.convs.4.weight\", \"net.visual_encoder.backbone.layer3.0.convs.4.bias\", \"net.visual_encoder.backbone.layer3.0.downsample.0.weight\", \"net.visual_encoder.backbone.layer3.0.downsample.1.weight\", \"net.visual_encoder.backbone.layer3.0.downsample.1.bias\", \"net.visual_encoder.backbone.layer3.1.convs.0.weight\", \"net.visual_encoder.backbone.layer3.1.convs.1.weight\", \"net.visual_encoder.backbone.layer3.1.convs.1.bias\", \"net.visual_encoder.backbone.layer3.1.convs.3.weight\", \"net.visual_encoder.backbone.layer3.1.convs.4.weight\", \"net.visual_encoder.backbone.layer3.1.convs.4.bias\", \"net.visual_encoder.backbone.layer4.0.convs.0.weight\", \"net.visual_encoder.backbone.layer4.0.convs.1.weight\", \"net.visual_encoder.backbone.layer4.0.convs.1.bias\", \"net.visual_encoder.backbone.layer4.0.convs.3.weight\", \"net.visual_encoder.backbone.layer4.0.convs.4.weight\", \"net.visual_encoder.backbone.layer4.0.convs.4.bias\", \"net.visual_encoder.backbone.layer4.0.downsample.0.weight\", \"net.visual_encoder.backbone.layer4.0.downsample.1.weight\", \"net.visual_encoder.backbone.layer4.0.downsample.1.bias\", \"net.visual_encoder.backbone.layer4.1.convs.0.weight\", \"net.visual_encoder.backbone.layer4.1.convs.1.weight\", \"net.visual_encoder.backbone.layer4.1.convs.1.bias\", \"net.visual_encoder.backbone.layer4.1.convs.3.weight\", \"net.visual_encoder.backbone.layer4.1.convs.4.weight\", \"net.visual_encoder.backbone.layer4.1.convs.4.bias\", \"net.visual_encoder.compression.0.weight\", \"net.visual_encoder.compression.1.weight\", \"net.visual_encoder.compression.1.bias\", \"net.visual_fc.1.weight\", \"net.visual_fc.1.bias\", \"net.state_encoder.rnn.weight_ih_l0\", \"net.state_encoder.rnn.weight_hh_l0\", \"net.state_encoder.rnn.bias_ih_l0\", \"net.state_encoder.rnn.bias_hh_l0\", \"net.state_encoder.rnn.weight_ih_l1\", \"net.state_encoder.rnn.weight_hh_l1\", \"net.state_encoder.rnn.bias_ih_l1\", \"net.state_encoder.rnn.bias_hh_l1\", \"action_distribution.linear.weight\", \"action_distribution.linear.bias\", \"critic.fc.weight\", \"critic.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/anand/habitat-lab/examples/tutorials/Task2.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anand/habitat-lab/examples/tutorials/Task2.ipynb#W5sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m action_space \u001b[39m=\u001b[39m spaces\u001b[39m.\u001b[39mDiscrete(\u001b[39m4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anand/habitat-lab/examples/tutorials/Task2.ipynb#W5sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m policy \u001b[39m=\u001b[39m MyPolicy(observation_space\u001b[39m=\u001b[39mOBSERVATION_SPACES[\u001b[39m\"\u001b[39m\u001b[39mdepth_model\u001b[39m\u001b[39m\"\u001b[39m], action_space\u001b[39m=\u001b[39mACTION_SPACE)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anand/habitat-lab/examples/tutorials/Task2.ipynb#W5sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m policy\u001b[39m.\u001b[39;49mload_state_dict(pretrained_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anand/habitat-lab/examples/tutorials/Task2.ipynb#W5sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anand/habitat-lab/examples/tutorials/Task2.ipynb#W5sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m policy\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/habitat/lib/python3.9/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MyPolicy:\n\tMissing key(s) in state_dict: \"conv.weight\", \"conv.bias\", \"resnet.conv1.weight\", \"resnet.bn1.weight\", \"resnet.bn1.bias\", \"resnet.bn1.running_mean\", \"resnet.bn1.running_var\", \"resnet.layer1.0.conv1.weight\", \"resnet.layer1.0.bn1.weight\", \"resnet.layer1.0.bn1.bias\", \"resnet.layer1.0.bn1.running_mean\", \"resnet.layer1.0.bn1.running_var\", \"resnet.layer1.0.conv2.weight\", \"resnet.layer1.0.bn2.weight\", \"resnet.layer1.0.bn2.bias\", \"resnet.layer1.0.bn2.running_mean\", \"resnet.layer1.0.bn2.running_var\", \"resnet.layer1.1.conv1.weight\", \"resnet.layer1.1.bn1.weight\", \"resnet.layer1.1.bn1.bias\", \"resnet.layer1.1.bn1.running_mean\", \"resnet.layer1.1.bn1.running_var\", \"resnet.layer1.1.conv2.weight\", \"resnet.layer1.1.bn2.weight\", \"resnet.layer1.1.bn2.bias\", \"resnet.layer1.1.bn2.running_mean\", \"resnet.layer1.1.bn2.running_var\", \"resnet.layer2.0.conv1.weight\", \"resnet.layer2.0.bn1.weight\", \"resnet.layer2.0.bn1.bias\", \"resnet.layer2.0.bn1.running_mean\", \"resnet.layer2.0.bn1.running_var\", \"resnet.layer2.0.conv2.weight\", \"resnet.layer2.0.bn2.weight\", \"resnet.layer2.0.bn2.bias\", \"resnet.layer2.0.bn2.running_mean\", \"resnet.layer2.0.bn2.running_var\", \"resnet.layer2.0.downsample.0.weight\", \"resnet.layer2.0.downsample.1.weight\", \"resnet.layer2.0.downsample.1.bias\", \"resnet.layer2.0.downsample.1.running_mean\", \"resnet.layer2.0.downsample.1.running_var\", \"resnet.layer2.1.conv1.weight\", \"resnet.layer2.1.bn1.weight\", \"resnet.layer2.1.bn1.bias\", \"resnet.layer2.1.bn1.running_mean\", \"resnet.layer2.1.bn1.running_var\", \"resnet.layer2.1.conv2.weight\", \"resnet.layer2.1.bn2.weight\", \"resnet.layer2.1.bn2.bias\", \"resnet.layer2.1.bn2.running_mean\", \"resnet.layer2.1.bn2.running_var\", \"resnet.layer3.0.conv1.weight\", \"resnet.layer3.0.bn1.weight\", \"resnet.layer3.0.bn1.bias\", \"resnet.layer3.0.bn1.running_mean\", \"resnet.layer3.0.bn1.running_var\", \"resnet.layer3.0.conv2.weight\", \"resnet.layer3.0.bn2.weight\", \"resnet.layer3.0.bn2.bias\", \"resnet.layer3.0.bn2.running_mean\", \"resnet.layer3.0.bn2.running_var\", \"resnet.layer3.0.downsample.0.weight\", \"resnet.layer3.0.downsample.1.weight\", \"resnet.layer3.0.downsample.1.bias\", \"resnet.layer3.0.downsample.1.running_mean\", \"resnet.layer3.0.downsample.1.running_var\", \"resnet.layer3.1.conv1.weight\", \"resnet.layer3.1.bn1.weight\", \"resnet.layer3.1.bn1.bias\", \"resnet.layer3.1.bn1.running_mean\", \"resnet.layer3.1.bn1.running_var\", \"resnet.layer3.1.conv2.weight\", \"resnet.layer3.1.bn2.weight\", \"resnet.layer3.1.bn2.bias\", \"resnet.layer3.1.bn2.running_mean\", \"resnet.layer3.1.bn2.running_var\", \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\", \"resnet.fc.weight\", \"resnet.fc.bias\", \"fc1.weight\", \"fc1.bias\", \"rnn.weight_ih_l0\", \"rnn.weight_hh_l0\", \"rnn.bias_ih_l0\", \"rnn.bias_hh_l0\", \"rnn.weight_ih_l1\", \"rnn.weight_hh_l1\", \"rnn.bias_ih_l1\", \"rnn.bias_hh_l1\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"net.prev_action_embedding.weight\", \"net.tgt_embeding.weight\", \"net.tgt_embeding.bias\", \"net.visual_encoder.backbone.conv1.0.weight\", \"net.visual_encoder.backbone.conv1.1.weight\", \"net.visual_encoder.backbone.conv1.1.bias\", \"net.visual_encoder.backbone.layer1.0.convs.0.weight\", \"net.visual_encoder.backbone.layer1.0.convs.1.weight\", \"net.visual_encoder.backbone.layer1.0.convs.1.bias\", \"net.visual_encoder.backbone.layer1.0.convs.3.weight\", \"net.visual_encoder.backbone.layer1.0.convs.4.weight\", \"net.visual_encoder.backbone.layer1.0.convs.4.bias\", \"net.visual_encoder.backbone.layer1.1.convs.0.weight\", \"net.visual_encoder.backbone.layer1.1.convs.1.weight\", \"net.visual_encoder.backbone.layer1.1.convs.1.bias\", \"net.visual_encoder.backbone.layer1.1.convs.3.weight\", \"net.visual_encoder.backbone.layer1.1.convs.4.weight\", \"net.visual_encoder.backbone.layer1.1.convs.4.bias\", \"net.visual_encoder.backbone.layer2.0.convs.0.weight\", \"net.visual_encoder.backbone.layer2.0.convs.1.weight\", \"net.visual_encoder.backbone.layer2.0.convs.1.bias\", \"net.visual_encoder.backbone.layer2.0.convs.3.weight\", \"net.visual_encoder.backbone.layer2.0.convs.4.weight\", \"net.visual_encoder.backbone.layer2.0.convs.4.bias\", \"net.visual_encoder.backbone.layer2.0.downsample.0.weight\", \"net.visual_encoder.backbone.layer2.0.downsample.1.weight\", \"net.visual_encoder.backbone.layer2.0.downsample.1.bias\", \"net.visual_encoder.backbone.layer2.1.convs.0.weight\", \"net.visual_encoder.backbone.layer2.1.convs.1.weight\", \"net.visual_encoder.backbone.layer2.1.convs.1.bias\", \"net.visual_encoder.backbone.layer2.1.convs.3.weight\", \"net.visual_encoder.backbone.layer2.1.convs.4.weight\", \"net.visual_encoder.backbone.layer2.1.convs.4.bias\", \"net.visual_encoder.backbone.layer3.0.convs.0.weight\", \"net.visual_encoder.backbone.layer3.0.convs.1.weight\", \"net.visual_encoder.backbone.layer3.0.convs.1.bias\", \"net.visual_encoder.backbone.layer3.0.convs.3.weight\", \"net.visual_encoder.backbone.layer3.0.convs.4.weight\", \"net.visual_encoder.backbone.layer3.0.convs.4.bias\", \"net.visual_encoder.backbone.layer3.0.downsample.0.weight\", \"net.visual_encoder.backbone.layer3.0.downsample.1.weight\", \"net.visual_encoder.backbone.layer3.0.downsample.1.bias\", \"net.visual_encoder.backbone.layer3.1.convs.0.weight\", \"net.visual_encoder.backbone.layer3.1.convs.1.weight\", \"net.visual_encoder.backbone.layer3.1.convs.1.bias\", \"net.visual_encoder.backbone.layer3.1.convs.3.weight\", \"net.visual_encoder.backbone.layer3.1.convs.4.weight\", \"net.visual_encoder.backbone.layer3.1.convs.4.bias\", \"net.visual_encoder.backbone.layer4.0.convs.0.weight\", \"net.visual_encoder.backbone.layer4.0.convs.1.weight\", \"net.visual_encoder.backbone.layer4.0.convs.1.bias\", \"net.visual_encoder.backbone.layer4.0.convs.3.weight\", \"net.visual_encoder.backbone.layer4.0.convs.4.weight\", \"net.visual_encoder.backbone.layer4.0.convs.4.bias\", \"net.visual_encoder.backbone.layer4.0.downsample.0.weight\", \"net.visual_encoder.backbone.layer4.0.downsample.1.weight\", \"net.visual_encoder.backbone.layer4.0.downsample.1.bias\", \"net.visual_encoder.backbone.layer4.1.convs.0.weight\", \"net.visual_encoder.backbone.layer4.1.convs.1.weight\", \"net.visual_encoder.backbone.layer4.1.convs.1.bias\", \"net.visual_encoder.backbone.layer4.1.convs.3.weight\", \"net.visual_encoder.backbone.layer4.1.convs.4.weight\", \"net.visual_encoder.backbone.layer4.1.convs.4.bias\", \"net.visual_encoder.compression.0.weight\", \"net.visual_encoder.compression.1.weight\", \"net.visual_encoder.compression.1.bias\", \"net.visual_fc.1.weight\", \"net.visual_fc.1.bias\", \"net.state_encoder.rnn.weight_ih_l0\", \"net.state_encoder.rnn.weight_hh_l0\", \"net.state_encoder.rnn.bias_ih_l0\", \"net.state_encoder.rnn.bias_hh_l0\", \"net.state_encoder.rnn.weight_ih_l1\", \"net.state_encoder.rnn.weight_hh_l1\", \"net.state_encoder.rnn.bias_ih_l1\", \"net.state_encoder.rnn.bias_hh_l1\", \"action_distribution.linear.weight\", \"action_distribution.linear.bias\", \"critic.fc.weight\", \"critic.fc.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from gym import spaces\n",
    "\n",
    "ACTION_SPACE = spaces.Discrete(4)\n",
    "\n",
    "OBSERVATION_SPACES = {\n",
    "    \"depth_model\": spaces.Dict(\n",
    "        {\n",
    "            \"depth\": spaces.Box(\n",
    "                low=0,\n",
    "                high=1,\n",
    "                shape=(224, 224, 1),\n",
    "                dtype=np.float32,\n",
    "            ),\n",
    "            \"pointgoal_with_gps_compass\": spaces.Box(\n",
    "                low=np.finfo(np.float32).min,\n",
    "                high=np.finfo(np.float32).max,\n",
    "                shape=(2,),\n",
    "                dtype=np.float32,\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "}\n",
    "class MyPolicy(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, hidden_size=512, num_recurrent_layers=2, rnn_type=\"LSTM\", resnet_baseplanes=32, backbone=\"resnet18\", normalize_visual_inputs=False, force_blind_policy=False, policy_config=None, aux_loss_config={}, fuse_keys=None, **kwargs):\n",
    "        super(MyPolicy, self).__init__()\n",
    "\n",
    "        # Define the components of the policy\n",
    "        self.conv = nn.Conv2d(observation_space['depth'].shape[2], resnet_baseplanes, kernel_size=3, stride=2, padding=1)\n",
    "        self.resnet = models.resnet18(pretrained=True)  # Assuming you want to use a pretrained ResNet18\n",
    "        self.fc1 = nn.Linear(resnet_baseplanes * 4 * 4 + 2, hidden_size)\n",
    "        self.rnn = getattr(nn, rnn_type)(hidden_size, hidden_size, num_recurrent_layers, batch_first=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_space.n)  # Use action_space.n instead of action_space\n",
    "\n",
    "    def forward(self, obs, rnn_hidden_states=None, masks=None):\n",
    "        depth_image = obs['head_depth']\n",
    "        pointgoal_with_gps_compass = obs['relative_resting_position']\n",
    "\n",
    "        # Process the depth image through the convolutional layers\n",
    "        x = F.relu(self.conv(depth_image))\n",
    "        x = F.avg_pool2d(x, 4)  # Assuming the output size is 4x4\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        # Process the pointgoal_with_gps_compass\n",
    "        x = torch.cat((x, pointgoal_with_gps_compass), dim=1)\n",
    "\n",
    "        # Pass through fully connected and recurrent layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if rnn_hidden_states is not None:\n",
    "            x, rnn_hidden_states = self.rnn(x.unsqueeze(0), rnn_hidden_states)\n",
    "            x = x.squeeze(0)\n",
    "        else:\n",
    "            x, rnn_hidden_states = self.rnn(x.unsqueeze(0))\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Output the action logits\n",
    "        action_logits = self.fc2(x)\n",
    "\n",
    "        return action_logits, rnn_hidden_states\n",
    "\n",
    "# Create an instance of MyPolicy\n",
    "MODELS = {\n",
    "    \"pointnav_weights.pth\": {\n",
    "        \"backbone\": \"resnet18\",\n",
    "        \"observation_space\": OBSERVATION_SPACES[\"depth_model\"],\n",
    "        \"action_space\": ACTION_SPACE,\n",
    "    }}\n",
    "model_weights_path = 'data/new_checkpoints/pointnav_weights.pth'  # Replace with the actual path to your model weights file\n",
    "\n",
    "pretrained_state = torch.load(model_weights_path, map_location=\"cpu\")\n",
    "\n",
    "observation_space = {'depth': torch.rand(1, 224, 224, 1), 'pointgoal_with_gps_compass': torch.rand(1, 2)}\n",
    "action_space = spaces.Discrete(4)\n",
    "policy = MyPolicy(observation_space=OBSERVATION_SPACES[\"depth_model\"], action_space=ACTION_SPACE)\n",
    "\n",
    "policy.load_state_dict(pretrained_state)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "policy.eval()\n",
    "obs = env.reset()  \n",
    "\n",
    "action_logits, rnn_hidden_states = policy(obs)\n",
    "print(action_logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "action = None\n",
    "obs = env.reset()\n",
    "# print(obs[\"head_rgb\"])\n",
    "valid_actions = [\"stop\",\"move_forward\",\"turn_left\", \"turn_right\"]\n",
    "interactive_control = False  # @param {type:\"boolean\"}\n",
    "while action != \"stop\":\n",
    "    display_sample(obs[\"head_rgb\"])\n",
    "    print(\n",
    "        \"distance to goal: {:.2f}\".format(\n",
    "            obs[\"ee_pos\"][0]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"angle to goal (radians): {:.2f}\".format(\n",
    "            obs[\"ee_pos\"][1]\n",
    "        )\n",
    "    )\n",
    "    obs = env.step(\n",
    "        {\n",
    "            \"action\": action,\n",
    "        }\n",
    "    )\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
